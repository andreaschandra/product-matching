{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fine-tune BERT MRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108,311,810\n"
     ]
    }
   ],
   "source": [
    "print(f\"{num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = pd.read_csv('../data/text_clean/train.csv')\n",
    "d_test = pd.read_csv('../data/text_clean/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference using BERT MRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = d_train.title_1_pre.to_list()\n",
    "t2 = d_train.title_2_pre.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [18:07<00:00, 27.20s/it]\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "end = 256\n",
    "y_pred_batch = []\n",
    "for batch_index in tqdm(range(int(len(t1)/256) + 1)):\n",
    "    t1_batch = t1[start:end]\n",
    "    t2_batch = t2[start:end]\n",
    "    \n",
    "    title_encode = tokenizer(t1_batch, t2_batch, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    y_pred = model(**title_encode)\n",
    "    y_pred = torch.softmax(y_pred[0], dim=1).argmax(1)\n",
    "    \n",
    "    y_pred_batch.extend(y_pred.tolist())\n",
    "    \n",
    "    start += 256\n",
    "    end += 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(y_pred_batch)\n",
    "y_true = d_train.Label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f score 0.4233435022802566\n",
      "accuracy 0.4939581491305629\n"
     ]
    }
   ],
   "source": [
    "print(\"f score\", f1_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "print(\"accuracy\", accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(d_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4430\n",
       "0    3204\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1413\n",
       "0    1132\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShopeeDataset():\n",
    "    def __init__(self, data, test):\n",
    "        train, val = train_test_split(data, random_state=123)\n",
    "        \n",
    "        train.reset_index(drop=True, inplace=True)\n",
    "        val.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        self.dataset = {\n",
    "            'train': (train, train.shape[0]),\n",
    "            'val': (val, val.shape[0]),\n",
    "            'test': (test, test.shape[0])\n",
    "        }\n",
    "        \n",
    "        self.set_split(split='train')\n",
    "        \n",
    "    def set_split(self, split='train'):\n",
    "        self.data, self.length = self.dataset[split]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        t1 = self.data.loc[idx, 'title_1_pre']\n",
    "        t2 = self.data.loc[idx, 'title_2_pre']\n",
    "        label = self.data.loc[idx, 'Label']\n",
    "        \n",
    "        return t1, t2, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ShopeeDataset(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_encode(data):\n",
    "    t1, t2, label = list(zip(*data))\n",
    "    title_encode = tokenizer(t1, t2, return_tensors=\"pt\", padding=True)\n",
    "    label = torch.LongTensor(label)\n",
    "    \n",
    "    return title_encode, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y, y_pred):\n",
    "    \n",
    "    n_correct = torch.eq(y, y_pred).sum().item()\n",
    "    accuracy = (n_correct/y_pred.shape[0])*100\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | time: 49.43s\n",
      "\ttrain loss: 0.73 | train acc: 50.78\n",
      "\tval loss: 2.19 | val acc: 55.47\n",
      "epoch: 2 | time: 52.15s\n",
      "\ttrain loss: 1.93 | train acc: 57.81\n",
      "\tval loss: 0.73 | val acc: 44.53\n",
      "epoch: 3 | time: 51.79s\n",
      "\ttrain loss: 0.73 | train acc: 43.75\n",
      "\tval loss: 0.78 | val acc: 55.47\n",
      "epoch: 4 | time: 53.00s\n",
      "\ttrain loss: 0.76 | train acc: 57.81\n",
      "\tval loss: 0.72 | val acc: 55.47\n",
      "epoch: 5 | time: 51.87s\n",
      "\ttrain loss: 0.71 | train acc: 56.25\n",
      "\tval loss: 0.69 | val acc: 55.47\n",
      "epoch: 6 | time: 53.33s\n",
      "\ttrain loss: 0.69 | train acc: 57.81\n",
      "\tval loss: 0.70 | val acc: 44.53\n",
      "epoch: 7 | time: 52.16s\n",
      "\ttrain loss: 0.71 | train acc: 48.44\n",
      "\tval loss: 0.72 | val acc: 44.53\n",
      "epoch: 8 | time: 53.05s\n",
      "\ttrain loss: 0.76 | train acc: 40.62\n",
      "\tval loss: 0.70 | val acc: 44.53\n",
      "epoch: 9 | time: 52.73s\n",
      "\ttrain loss: 0.72 | train acc: 46.88\n",
      "\tval loss: 0.69 | val acc: 55.47\n",
      "epoch: 10 | time: 53.31s\n",
      "\ttrain loss: 0.69 | train acc: 55.47\n",
      "\tval loss: 0.70 | val acc: 55.47\n",
      "epoch: 11 | time: 52.04s\n",
      "\ttrain loss: 0.69 | train acc: 57.81\n",
      "\tval loss: 0.72 | val acc: 55.47\n",
      "epoch: 12 | time: 53.13s\n",
      "\ttrain loss: 0.69 | train acc: 57.81\n",
      "\tval loss: 0.72 | val acc: 55.47\n",
      "epoch: 13 | time: 52.04s\n",
      "\ttrain loss: 0.71 | train acc: 57.81\n",
      "\tval loss: 0.70 | val acc: 55.47\n",
      "epoch: 14 | time: 52.77s\n",
      "\ttrain loss: 0.69 | train acc: 57.81\n",
      "\tval loss: 0.69 | val acc: 55.47\n",
      "epoch: 15 | time: 52.33s\n",
      "\ttrain loss: 0.71 | train acc: 51.56\n",
      "\tval loss: 0.69 | val acc: 55.47\n",
      "epoch: 16 | time: 54.47s\n",
      "\ttrain loss: 0.72 | train acc: 47.66\n",
      "\tval loss: 0.69 | val acc: 44.53\n",
      "epoch: 17 | time: 52.54s\n",
      "\ttrain loss: 0.69 | train acc: 53.91\n",
      "\tval loss: 0.69 | val acc: 55.47\n",
      "epoch: 18 | time: 53.65s\n",
      "\ttrain loss: 0.69 | train acc: 53.91\n",
      "\tval loss: 0.69 | val acc: 55.47\n",
      "epoch: 19 | time: 52.53s\n",
      "\ttrain loss: 0.69 | train acc: 53.91\n",
      "\tval loss: 0.69 | val acc: 55.47\n",
      "epoch: 20 | time: 53.75s\n",
      "\ttrain loss: 0.69 | train acc: 56.25\n",
      "\tval loss: 0.70 | val acc: 55.47\n",
      "epoch: 21 | time: 52.30s\n",
      "\ttrain loss: 0.72 | train acc: 57.03\n",
      "\tval loss: 0.71 | val acc: 55.47\n",
      "epoch: 22 | time: 53.76s\n",
      "\ttrain loss: 0.71 | train acc: 57.81\n",
      "\tval loss: 0.70 | val acc: 55.47\n",
      "epoch: 23 | time: 52.35s\n",
      "\ttrain loss: 0.70 | train acc: 57.03\n",
      "\tval loss: 0.69 | val acc: 55.47\n",
      "epoch: 24 | time: 54.44s\n",
      "\ttrain loss: 0.67 | train acc: 60.94\n",
      "\tval loss: 0.69 | val acc: 55.47\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-25bfc21ce5e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mrunning_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrunning_acc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 101):\n",
    "    running_loss = 0\n",
    "    running_loss_v = 0\n",
    "    running_acc = 0\n",
    "    running_acc_v = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    dataset.set_split(split='train')\n",
    "    data_gen = DataLoader(dataset, batch_size=128, collate_fn=tokenizer_encode)\n",
    "    for batch_index, (X, y) in enumerate(data_gen, 1):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logit = model(**X)[0]\n",
    "        y_pred = torch.softmax(logit, dim=1).argmax(1)\n",
    "        \n",
    "        loss = criterion(logit, y)\n",
    "        running_loss += (loss.item() - running_loss) / batch_index\n",
    "        \n",
    "        accuracy = calculate_accuracy(y, y_pred)\n",
    "        running_acc += (accuracy - running_acc) / batch_index\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        break\n",
    "        \n",
    "    model.eval()\n",
    "    dataset.set_split(split='val')\n",
    "    data_gen = DataLoader(dataset, batch_size=128, collate_fn=tokenizer_encode)\n",
    "    for batch_index, (X, y) in enumerate(data_gen, 1):\n",
    "        logit = model(**X)[0]\n",
    "        y_pred = torch.softmax(logit, dim=1).argmax(1)\n",
    "        \n",
    "        loss = criterion(logit, y)\n",
    "        running_loss_v += (loss.item() - running_loss_v) / batch_index\n",
    "        \n",
    "        accuracy = calculate_accuracy(y, y_pred)\n",
    "        running_acc_v += (accuracy - running_acc_v) / batch_index\n",
    "        break\n",
    "        \n",
    "    duration = time.time() - start\n",
    "    print(f\"epoch: {epoch} | time: {duration:.2f}s\")\n",
    "    print(f\"\\ttrain loss: {running_loss:.2f} | train acc: {running_acc:.2f}\")\n",
    "    print(f\"\\tval loss: {running_loss_v:.2f} | val acc: {running_acc_v:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
